---
title: Airflow usage
author: ct
layout: post
categories:
  - airflow
tags:
  - airflow
  - python
---

### Airflow能做什么

Airflow是一个工作流分配管理系统。

### 安装和使用

* 最简单安装

	* `pip install airflow`
	* `pip install "airflow[crypto, password]"`

* 安装成功之后，就可以使用了，默认使用`SequentialExecutor`, 顺次执行任
  务。

  * 初始化数据库 `airflow initdb`
  * 启动web服务器 `airflow webserver -p 8080`
  * 启动任务 `airflow scheduler` 
  * 或者测试文章末尾的DAG `airflow test ct1 print_date 2016-05-14`

### 配置 `mysql`以启用`LocalExecutor`

* 安装mysql数据库支持
	* `yum install mysql mysql-server`
	* `pip install airflow[mysql]`

* 设置mysql根用户的密码
	* `mysql -uroot`登录mysql数据库
	* 在mysql操作界面依次输入sql语句 
	
	  `SET PASSWORD=PASSWORD("passwd");`
      
	  `FLUSH PRIVILEGES;`
	  
* 新建用户和数据库
	* 新建名字为`airflow`的数据库 `CREATE DATABASE airflow;`
	* 新建用户`ct`，密码为`152108`, 该用户对数据网`airflow`有完全操作
	  权限
	  
	  `GRANT all privileges on airflow.* TO 'ct'@'localhost'  IDENTIFIED BY '152108';` 
	  
	  `FLUSH PRIVILEGES;`

	* 新建数据库`celery_result_airflow`用于`celery_result_backend` 
      (这一步后来没用到，可以略过)

	  `mysql -uct -p152108` 
	  
	  `CREATE DATABASE celery_result_airflow;`

* 修改airflow配置文件支持mysql
	* `airflow.cfg` 文件通常在`~/airflow`目录下
	* 更改数据库链接： 
	  
		sql_alchemy_conn = mysql://ct:152108@localhost/airflow
		
		对应字段解释如下：

		dialect+driver://username:password@host:port/database

	* 初始化数据库 `airflow initdb`
	* 初始化数据库成功后，可进入mysql查看新生成的数据表。

		mysql -uct -p152108

		USE airflow;

		SHOW TABLES;

### 使用LocalExecutor （可选方案一）

#### 配置LocalExecutor

* 修改airflow配置文件

	* `airflow.cfg` 文件通常在`~/airflow`目录下

	* 更改executor为 `executor = LocalExecutor`

#### 测试

* 在`~/airflow/dags`下新建几个task文件，具体见TASK
* 顺次执行下面的命令
	* `airflow initdb` (若前面执行过，就跳过)
	* `airflow webserver --debug &`
	* `airflow scheduler`

* 打开网页查看运行进程


### 使用CeleryExecutor (可选方案二)

#### 配置celery+rabbitmq支持
	
* 安装celery和rabbitmq
	* `pip install airflow[celery]`
	* `pip install airflow[rabbitmq]`

	* 安装erlang和rabbitmq (Centos 6,[REF](http://www.rabbitmq.com/install-rpm.html)): 

		wget https://packages.erlang-solutions.com/erlang/esl-erlang/FLAVOUR_1_general/esl-erlang_18.3-1~centos~6_amd64.rpm

		yum install esl-erlang_18.3-1~centos~6_amd64.rpm

		wget https://github.com/jasonmcintosh/esl-erlang-compat/releases/download/1.1.1/esl-erlang-compat-18.1-1.noarch.rpm

		yum install esl-erlang-compat-18.1-1.noarch.rpm

		wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.1/rabbitmq-server-3.6.1-1.noarch.rpm

		yum install rabbitmq-server-3.6.1-1.noarch.rpm

* 配置rabbitmq

	* 启动rabbitmq: `rabbitmq-server -detached`
	* 开机启动rabbitmq: `chkconfig rabbitmq-server on`
	* 配置rabbitmq ([REF](http://docs.celeryproject.org/en/latest/getting-started/brokers/rabbitmq.html))

		rabbitmqctl add_user ct 152108

		rabbitmqctl add_vhost ct_airflow

		rabbitmqctl set_user_tags ct airflow

		rabbitmqctl set_permissions -p ct_airflow ct ".*" ".*" ".*"

		rabbitmq-plugins enable rabbitmq_management # no usage

* 修改airflow配置文件支持Celery

	* `airflow.cfg` 文件通常在`~/airflow`目录下

	* 更改executor为 `executor = CeleryExecutor`

	* 更改[broker_url](http://docs.celeryproject.org/en/latest/getting-started/brokers/rabbitmq.html)

	  `broker_url = 'amqp://ct:152108@localhost:5672/ct_airflow'`

	  `Format explanation: transport://userid:password@hostname:port/virtual_host`
	
	* 更改[celery_result_backend](http://docs.celeryproject.org/en/latest/configuration.html#conf-database-result-backend), 
	
	  `celery_result_backend = 'amqp://ct:152108@localhost:5672/ct_airflow'`

	  `Format explanation: transport://userid:password@hostname:port/virtual_host`

	  `可以与broker_url相同`

#### 测试

* 启动服务器：`airflow webserver --debug`
* 启动celery worker (不能用根用户)：`airflow worker`
* 启动scheduler: `airflow scheduler`
* 提示：
	* 测试过程中注意观察运行上面3个命令的3个窗口输出的日志
	* 当遇到不符合常理的情况时考虑清空 `airflow backend`的数据库


### airflow.cfg 其它修改

* dags_folder
  
  `dags_folder`目录支持子目录和软连接，因此不同的dag可以分门别类的存储
  起来。

* 设置邮件发送服务 

    smtp_host = smtp.163.com

	smtp_starttls = True

	smtp_ssl = False
	
	smtp_user = username@163.com
		
	smtp_port = 25
	
	smtp_password = userpasswd
	
	smtp_mail_from = username@163.com
  
* 多用户登录设置 (似乎只有CeleryExecutor支持)

    * 修改`airflow.cfg`中的下面3行配置

        authenticate = True

	    auth_backend = airflow.contrib.auth.backends.password_auth
  
	    filter_by_owner = True

    * 增加一个用户
		
		import airflow

		from airflow import models,   settings
		
		from airflow.contrib.auth.backends.password_auth import PasswordUser
		
		user = PasswordUser(models.User())

		user.username = 'ehbio'

		user.email = 'mail@ehbio.com'

		user.password = 'ehbio'

		session = settings.Session()

		session.add(user)

		session.commit()

		session.close()

		exit()


### TASK

* 参数解释
	* `depends_on_past`
	   
	  Airflow assumes idempotent tasks that operate on immutable data
	  chunks. It also assumes that all task instance (each task for each
	  schedule) needs to run.
		
	  If your tasks need to be executed sequentially,  you need to
	  tell Airflow: use the `depends_on_past=True` flag on the tasks
	  that require sequential execution.)

	* `timestamp` in format like `2016-01-01T00:03:00`

	* Task中调用的命令出错后需要在网站`Graph view`中点击`run`手动重启。
	  为了方便任务修改后的顺利运行，有个折衷的方法是：

	  * 设置 `'email_on_retry': True`
	  * 设置较长的`retry_delay`，方便在收到邮件后，立即做出处理
	  * 然后再修改为较短的`retry_delay`，方便快速启动

* 写完task DAG后，一定记得先检测下有无语法错误 `python dag.py`

* 测试文件1：ct1.py

```
from airflow import DAG
from airflow.operators import BashOperator, MySqlOperator

from datetime import datetime, timedelta

one_min_ago = datetime.combine(datetime.today() -
	timedelta(minutes=1), datetime.min.time())

default_args = {
    'owner': 'airflow',         
		
	#为了测试方便，起始时间一般为当前时间减去schedule_interval
    'start_date': datatime(2016, 5, 29, 8, 30), 
    'email': ['chentong_biology@163.com'],
    'email_on_failure': False, 
    'email_on_retry': False, 
	'depends_on_past': False, 
    'retries': 1, 
    'retry_delay': timedelta(minutes=5), 
    #'queue': 'bash_queue',
    #'pool': 'backfill', 
    #'priority_weight': 10, 
	#'end_date': datetime(2016, 5, 29, 11, 30), 
}

# DAG id 'ct1'必须是unique的, 一般与文件名相同
dag = DAG('ct1', default_args=default_args,
    schedule_interval="@once")

t1 = BashOperator(
    task_id='print_date', 
    bash_command='date', 
    dag=dag)

#cmd = "/home/test/test.bash " 注意末尾的空格
t2 = BashOperator(
    task_id='echo', 
    bash_command='echo "test" ', 
    retries=3, 
    dag=dag)

templated_command = """
    {% for i in range(2) %}
        echo "{{ ds }}" 
        echo "{{ macros.ds_add(ds, 7) }}"
        echo "{{ params.my_param }}"
    {% endfor %}
"""
t3 = BashOperator(
    task_id='templated', 
    bash_command=templated_command, 
    params={'my_param': "Parameter I passed in"}, 
    dag=dag)

# This means that t2 will depend on t1 running successfully to run
# It is equivalent to t1.set_downstream(t2)
t2.set_upstream(t1)


t3.set_upstream(t1)

# all of this is equivalent to
# dag.set_dependency('print_date', 'sleep')
# dag.set_dependency('print_date', 'templated')
```

* 测试文件2: `ct2.py`

```
from airflow import DAG
from airflow.operators import BashOperator

from datetime import datetime, timedelta

one_min_ago = datetime.combine(datetime.today() - timedelta(minutes=1),
                                  datetime.min.time())

default_args = {
    'owner': 'airflow',         
    'depends_on_past': True, 
    'start_date': one_min_ago,
    'email': ['chentong_biology@163.com'],
    'email_on_failure': True, 
    'email_on_retry': True, 
    'retries': 5, 
    'retry_delay': timedelta(hours=30), 
    #'queue': 'bash_queue',
    #'pool': 'backfill', 
    #'priority_weight': 10, 
	#'end_date': datetime(2016, 5, 29, 11, 30), 
}

dag = DAG('ct2', default_args=default_args,
    schedule_interval="@once")

t1 = BashOperator(
    task_id='run1', 
    bash_command='(cd /home/ct/test; bash run1.sh -f ct_t1) ', 
    dag=dag)

t2 = BashOperator(
    task_id='run2', 
    bash_command='(cd /home/ct/test; bash run2.sh -f ct_t1) ', 
    dag=dag)

t2.set_upstream(t1)

```

* run1.sh 

```
#!/bin/bash

#set -x
set -e
set -u

usage()
{
cat <<EOF
${txtcyn}
Usage:

$0 options${txtrst}

${bldblu}Function${txtrst}:

This script is used to do ********************.

${txtbld}OPTIONS${txtrst}:
	-f	Data file ${bldred}[NECESSARY]${txtrst}
	-z	Is there a header[${bldred}Default TRUE${txtrst}]
EOF
}

file=
header='TRUE'

while getopts "hf:z:" OPTION
do
	case $OPTION in
		h)
			usage
			exit 1
			;;
		f)
			file=$OPTARG
			;;
		z)
			header=$OPTARG
			;;
		?)
			usage
			exit 1
			;;
	esac
done

if [ -z $file ]; then
	usage
	exit 1
fi

cat <<END >$file
A
B
C
D
E
F
G
END

sleep 20s
```

* run2.sh

```
#!/bin/bash

#set -x
set -e
set -u

usage()
{
cat <<EOF
${txtcyn}
Usage:

$0 options${txtrst}

${bldblu}Function${txtrst}:

This script is used to do ********************.

${txtbld}OPTIONS${txtrst}:
	-f	Data file ${bldred}[NECESSARY]${txtrst}
EOF
}

file=
header='TRUE'

while getopts "hf:z:" OPTION
do
	case $OPTION in
		h)
			usage
			exit 1
			;;
		f)
			file=$OPTARG
			;;
		?)
			usage
			exit 1
			;;
	esac
done

if [ -z $file ]; then
	usage
	exit 1
fi

awk 'BEGIN{OFS=FS="\t"}{print $0, "53"}' $file >${file}.out

```


### 安装redis (最后没用到)

* http://download.redis.io/releases/redis-3.2.0.tar.gz
* `tar xvzf redis-3.2.0.tar.gz` and `make`
* `redis-server`启动redis
* 使用`ps -ef | grep 'redis'`检测后台进程是否存在
* 检测6379端口是否在监听`netstat -lntp | grep 6379`


### References

1. [https://pythonhosted.org/airflow/](https://pythonhosted.org/airflow/)
2. <http://kintoki.farbox.com/post/ji-chu-zhi-shi/airflow>
3. <http://www.jianshu.com/p/59d69981658a>
4. <http://bytepawn.com/luigi-airflow-pinball.html>
5. <https://github.com/airbnb/airflow>
6. <https://media.readthedocs.org/pdf/airflow/latest/airflow.pdf>
7. <http://www.csdn.net/article/1970-01-01/2825690>
8. <http://www.cnblogs.com/harrychinese/p/airflow.html>
9. <https://segmentfault.com/a/1190000005078547>
