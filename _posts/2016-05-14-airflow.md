---
title: Airflow usage
author: ct
layout: post
categories:
  - airflow
tags:
  - airflow
  - python
---

### 安装

* 最简单安装

	* `pip install airflow`
	* `pip install airflow[crypto]`
	* `pip install airflow[password]`


### 配置 mysql

* 安装mysql数据库支持
	* `yum install mysql mysql-server`
	* `pip install airflow[mysql]`

* 设置mysql根用户的密码
	* `mysql -uroot`登录mysql数据库
	* 在mysql操作界面依次输入sql语句 `SET PASSWORD=PASSWORD("passwd");`
      `FLUSH PRIVILEGES;`，每输入一条语句都按下回车键。

* 新建用户和数据库
	* `CREATE DATABASE airflow;`新建数据库名字为`airflow`。
	* 新建用户`ct`，密码为`152108`, 该用户对数据网`airflow`有完全操作
	  权限，`GRANT all privileges on airflow.* TO 'ct'@'localhost' 
	  IDENTIFIED BY '152108';` `FLUSH PRIVILEGES;`
	* 新建数据库`celery_result_airflow`用于`celery_result_backend`, 
	  `mysql -uct -p152108` `CREATE DATABASE celery_result_airflow;`

* 修改airflow配置文件支持mysql
	* `airflow.cfg` 文件通常在`~/airflow`目录下
	* 更改数据库链接： 
	  
	  > sql_alchemy_conn = mysql://ct:152108@localhost/airflow
	  > dialect+driver://username:password@host:port/database

	* 初始化数据库 `airflow initdb`
	* 初始化数据库成功后，可进入mysql查看新生成的数据表。

	  > mysql -uct -p152108
	  > USE airflow;
	  > SHOW TABLES;

### 使用LocalExecutor （可选方案一）

#### 配置使用LocalExecutor

* 修改airflow配置文件支持Celery

	* `airflow.cfg` 文件通常在`~/airflow`目录下

	* 更改executor, `executor = LocalExecutor`

#### 测试

* 在`~/airflow/dags`下新建几个task文件，具体见TASK
* `airflow initdb` (若前面执行过，就跳过)
* `airflow webserver --debug &`
* `airflow scheduler`

* 打开网页查看运行进程


### 使用CeleryExecutor (可选方案二)

#### 配置celery+rabbitmq支持
	
* 安装celery和rabbitmq
	* `pip install airflow[celery]`
	* `pip install airflow[rabbitmq]`

	* 安装erlang和rabbitmq (Centos 6,[REF](http://www.rabbitmq.com/install-rpm.html)): 

	  > wget https://packages.erlang-solutions.com/erlang/esl-erlang/FLAVOUR_1_general/esl-erlang_18.3-1~centos~6_amd64.rpm
	  > yum install esl-erlang_18.3-1~centos~6_amd64.rpm
	  > wget https://github.com/jasonmcintosh/esl-erlang-compat/releases/download/1.1.1/esl-erlang-compat-18.1-1.noarch.rpm
	  >	yum install esl-erlang-compat-18.1-1.noarch.rpm
	  >	wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.1/rabbitmq-server-3.6.1-1.noarch.rpm
	  >	yum install rabbitmq-server-3.6.1-1.noarch.rpm

* 配置rabbitmq

	* 启动rabbitmq: `rabbitmq-server -detached`
	* 开机启动rabbitmq: `chkconfig rabbitmq-server on`
	* 配置rabbitmq ([REF](http://docs.celeryproject.org/en/latest/getting-started/brokers/rabbitmq.html))

	  > rabbitmqctl add_user ct 152108
	  >	rabbitmqctl add_vhost ct_airflow
	  >	rabbitmqctl set_user_tags ct airflow
	  >	rabbitmqctl set_permissions -p ct_airflow ct ".*" ".*" ".*"
	  >	rabbitmq-plugins enable rabbitmq_management # no usage

* 修改airflow配置文件支持Celery

	* `airflow.cfg` 文件通常在`~/airflow`目录下

	* 更改executor, `executor = CeleryExecutor`

	* 更改[broker_url](http://docs.celeryproject.org/en/latest/getting-started/brokers/rabbitmq.html), `broker_url = 'amqp://ct:152108@localhost:5672/ct_airflow'`
	
	* 更改[celery_result_backend](http://docs.celeryproject.org/en/latest/configuration.html#conf-database-result-backend), `celery_result_backend = db+mysql://ct:152108@localhost:3306/celery_result_airflow`



#### 测试

* `airflow webserver -p 8080`



### airflow.cfg 其它修改


### TASK

* ct1.py

```
from airflow import DAG
from airflow.operators import BashOperator, MySqlOperator

from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',         
    'depends_on_past': False, #这个是说如果项目是周期运行的，
							  #后面的项目是否要在前一个项目成功后再运
							  #行
		
	#为了测试方便，起始时间一般为当前时间减去schedule_interval
    'start_date': datetime(2016, 5, 27, 9, 0), 
    'email': ['chentong_biology@163.com'],
    'email_on_failure': False, 
    'email_on_retry': False, 
    'retries': 1, 
    'retry_delay': timedelta(minutes=5), 
    #'queue': 'bash_queue',
    #'pool': 'backfill', 
    #'priority_weight': 10, 
    'end_date': datetime(2016, 5, 29, 11, 30), 
}

# DAG id 'ct1'必须是unique的
dag = DAG('ct1', default_args=default_args,
    schedule_interval="@once")

t1 = BashOperator(
    task_id='print_date', 
    bash_command='date', 
    dag=dag)

#cmd = "/home/test/test.bash " 注意末尾的空格
t2 = BashOperator(
    task_id='mail', 
    bash_command='pythonmail3.py -s "test" ', 
    retries=3, 
    dag=dag)

templated_command = """
    {% for i in range(2) %}
        echo "{{ ds }}" 
        echo "{{ macros.ds_add(ds, 7) }}"
        echo "{{ params.my_param }}"
    {% endfor %}
"""
t3 = BashOperator(
    task_id='templated', 
    bash_command=templated_command, 
    params={'my_param': "Parameter I passed in"}, 
    dag=dag)

# This means that t2 will depend on t1 running successfully to run
# It is equivalent to t1.set_downstream(t2)
t2.set_upstream(t1)


t3.set_upstream(t1)

# all of this is equivalent to
# dag.set_dependency('print_date', 'sleep')
# dag.set_dependency('print_date', 'templated')
```

* ct2.py

```
from airflow import DAG
from airflow.operators import BashOperator, MySqlOperator

from datetime import datetime, timedelta

default_args = {
    'owner': 'ct',         
    'depends_on_past': True, 
    'start_date': datetime(2016, 5, 26, 9, 10), 
    'email': ['chentong_biology@163.com'],
    'email_on_failure': True, 
    'email_on_retry': False, 
    'retries': 1, 
    'retry_delay': timedelta(minutes=5), 
    #'queue': 'bash_queue',
    #'pool': 'backfill', 
    #'priority_weight': 10, 
    'end_date': datetime(2016, 5, 27, 11, 30), 
}

dag = DAG('ct2', default_args=default_args,
    schedule_interval=timedelta(seconds=30))

t1 = BashOperator(
    task_id='print_date', 
    bash_command='date', 
    dag=dag)

t2 = BashOperator(
    task_id='wrong_command', 
    bash_command='sleep___sdsdsdsdsd', 
    retries=2, 
    dag=dag)

# This means that t2 will depend on t1 running successfully to run
# It is equivalent to t1.set_downstream(t2)
t2.set_upstream(t1)

```

### References

1. [https://pythonhosted.org/airflow/](https://pythonhosted.org/airflow/)
2. <http://kintoki.farbox.com/post/ji-chu-zhi-shi/airflow>
3. <http://www.jianshu.com/p/59d69981658a>
4. <http://bytepawn.com/luigi-airflow-pinball.html>
5. <https://github.com/airbnb/airflow>
6. <https://media.readthedocs.org/pdf/airflow/latest/airflow.pdf>
7. <http://www.csdn.net/article/1970-01-01/2825690>

